<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CharacterLM • cntk</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">cntk</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/CharacterLM.html">CharacterLM</a>
    </li>
    <li>
      <a href="../articles/cifar10_example.html">CIFAR-10 Image Classification with Convolutional Neural Networks and CNTK</a>
    </li>
    <li>
      <a href="../articles/installation.html">Installing CNTK and the CNTK Package on Your System</a>
    </li>
    <li>
      <a href="../articles/mnist_example.html">Convolutional Neural Netowrks with MNIST</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Microsoft/CNTK-R">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>CharacterLM</h1>
            
            <h4 class="date">2017-11-28</h4>
          </div>

    
    
<div class="contents">
<div id="goals-and-overview" class="section level2">
<h2 class="hasAnchor">
<a href="#goals-and-overview" class="anchor"></a>Goals and Overview</h2>
<p>This example demonstrates how to build a neural character language model with CNTK using regular plaintext data.</p>
<p>A neural language model uses a recurrent neural network to predict words (or characters) with a richer context than traditional n-gram models allow. In this implementation, a character is run through an LSTM and the output is then put through a fully-connected layer to predict the next output character. The model can learn to be extremely expressive as the context is progressively built-up with each letter run through the RNN. For even more expressiveness, we allow a stack of LSTMs where the output of each layer is put through the next layer as its input.</p>
<p>This example is inspired by Andrej Karpathy’s blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">“The Unreasonable Effectiveness of Recurrent Neural Networks”</a> and his accompanying code at <a href="https://github.com/karpathy/char-rnn" class="uri">https://github.com/karpathy/char-rnn</a>. This example allows you to achieve similar results to those displayed in Karpathy’s blog, but with the packed-sequence training efficiency that CNTK allows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hidden_dim &lt;-<span class="st"> </span><span class="dv">256</span>
num_layers &lt;-<span class="st"> </span><span class="dv">2</span>
minibatch_size &lt;-<span class="st"> </span><span class="dv">100</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_data &lt;-<span class="st"> </span><span class="cf">function</span>(p, minibatch_size, data, char_to_ix, vocab_dim) {
  xi &lt;-<span class="st"> </span><span class="kw">c</span>()
  yi &lt;-<span class="st"> </span><span class="kw">c</span>()
  
  <span class="co"># the character LM predicts the next character so get sequences offset by 1</span>
  <span class="cf">for</span> (i <span class="cf">in</span> p <span class="op">:</span><span class="st"> </span>p <span class="op">+</span><span class="st"> </span>minibatch_size) {
    xi &lt;-<span class="st"> </span><span class="kw">c</span>(xi, char_to_ix[data[i]])
    yi &lt;-<span class="st"> </span><span class="kw">c</span>(yi, char_to_ix[data[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]])
  }
  
  <span class="co"># produce one-hot vectors</span>
  X &lt;-<span class="st"> </span><span class="kw">sparseMatrix</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(xi), xi) <span class="op">*</span><span class="st"> </span><span class="dv">1</span> <span class="co"># * 1 to make numeric</span>
  Y &lt;-<span class="st"> </span><span class="kw">sparseMatrix</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(yi), yi) <span class="op">*</span><span class="st"> </span><span class="dv">1</span>
  
  <span class="co"># return a list of matrices for each of X (features) and Y (labels)</span>
  <span class="kw">list</span>(<span class="kw">list</span>(X), <span class="kw">list</span>(Y))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample &lt;-<span class="st"> </span><span class="cf">function</span>(root, ix_to_char, vocab_dim, char_to_ix, <span class="dt">prime_text =</span> <span class="st">''</span>,
                   <span class="dt">use_hardmax =</span> <span class="ot">TRUE</span>, <span class="dt">length =</span> <span class="dv">100</span>, <span class="dt">temperature =</span> <span class="dv">1</span>) {
  apply_temp &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
    p &lt;-<span class="st"> </span>p<span class="op">^</span>temperature
    p <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(p)
  }
  sample_word &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
    <span class="cf">if</span> (use_hardmax) {
      <span class="kw">return</span>(<span class="kw">exp</span>(p) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">exp</span>(p)))
    }
    <span class="co"># normalize probabilities and then take weighted sample</span>
    p &lt;-<span class="st"> </span><span class="kw">exp</span>(p) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">exp</span>(p))
    p &lt;-<span class="st"> </span><span class="kw">apply_temp</span>(p)
    <span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(vocab_dim))
  }
  
  plen &lt;-<span class="st"> </span><span class="dv">1</span>
  prime &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">1</span>
  
  <span class="co"># start sequence with first input</span>
  x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol =</span> vocab_dim)
  <span class="cf">if</span> (prime_text <span class="op">!=</span><span class="st"> ''</span>) {
    plen &lt;-<span class="st"> </span><span class="kw">length</span>(prime_text)
    prime &lt;-<span class="st"> </span>char_to_ix[prime_text[<span class="dv">0</span>]]
  } <span class="cf">else</span> {
    prime &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>vocab_dim)
  }
  
  x[prime] &lt;-<span class="st"> </span><span class="dv">1</span>
  arguments &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">list</span>(x), <span class="kw">list</span>(<span class="ot">TRUE</span>))
  
  <span class="co"># setup a vector for the output characters and add the initial prime text</span>
  output &lt;-<span class="st"> </span><span class="kw">c</span>(prime)
  
  <span class="co"># loop through prime text</span>
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>plen) {
    p &lt;-<span class="st"> </span>root <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../reference/func_eval.html">func_eval</a></span>(arguments)
    
    <span class="co"># reset</span>
    x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol =</span> vocab_dim)
    <span class="cf">if</span> (i <span class="op">&lt;</span><span class="st"> </span>plen <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) {
      idx &lt;-<span class="st"> </span>char_to_ix[prime_text[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]]
    } <span class="cf">else</span> {
      idx &lt;-<span class="st"> </span><span class="kw">sample_word</span>(p)
    }
    
    output &lt;-<span class="st"> </span><span class="kw">c</span>(output, idx)
    x[idx] &lt;-<span class="st"> </span><span class="dv">1</span>
    arguments =<span class="st"> </span><span class="kw">list</span>(<span class="kw">list</span>(x), <span class="kw">list</span>(<span class="ot">TRUE</span>))
  }
  
  <span class="co"># loop through length of generated text, sampling along the way</span>
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>length<span class="op">-</span>plen) {
    p &lt;-<span class="st"> </span>root <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">eval</span>(arguments)
    idx &lt;-<span class="st"> </span><span class="kw">sample_word</span>(p)
    output &lt;-<span class="st"> </span><span class="kw">c</span>(output, idx)
    
    x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol =</span> vocab_dim)
    x[idx] &lt;-<span class="st"> </span><span class="dv">1</span>
    arguments &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">list</span>(x), <span class="kw">list</span>(<span class="ot">FALSE</span>))
  }
  
  <span class="co"># convert numeric representation back to characters</span>
  chars &lt;-<span class="st"> </span><span class="kw">c</span>()
  <span class="cf">for</span> (char <span class="cf">in</span> output) {
    chars &lt;-<span class="st"> </span><span class="kw">c</span>(chars, ix_to_char[<span class="kw">toString</span>(char)])
  }
  
  <span class="kw">paste</span>(chars, <span class="dt">collapse =</span> <span class="st">''</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">load_data_and_vocab &lt;-<span class="st"> </span><span class="cf">function</span>(path) {
  <span class="co"># load data</span>
  data &lt;-<span class="st"> </span><span class="kw">readChar</span>(path, <span class="kw">file.info</span>(path)<span class="op">$</span>size)[[<span class="dv">1</span>]]
  chars &lt;-<span class="st"> </span><span class="kw">unique</span>(data)
  data_size &lt;-<span class="st"> </span><span class="kw">length</span>(data)
  vocab_size &lt;-<span class="st"> </span><span class="kw">length</span>(chars)
  <span class="kw">sprintf</span>(<span class="st">"data has %d characters, %d unique"</span>, data_size, vocab_size)
  
  char_to_ix &lt;-<span class="st"> </span><span class="kw">list</span>()
  ix_to_char &lt;-<span class="st"> </span><span class="kw">list</span>()
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(chars)) {
    char_to_ix[[ chars[i] ]] &lt;-<span class="st"> </span>i
    ix_to_char[[ <span class="kw">toString</span>(i) ]] &lt;-<span class="st"> </span>chars[i]
  }
  
  <span class="co"># write vocab for future use</span>
  <span class="kw">write</span>(chars, <span class="kw">paste</span>(path, <span class="st">".vocab"</span>, <span class="dt">sep =</span> <span class="st">''</span>))
  
  <span class="kw">list</span>(data, char_to_ix, ix_to_char, data_size, vocab_size)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">create_model &lt;-<span class="st"> </span><span class="cf">function</span>(output_dim) {
  <span class="kw"><a href="../reference/Sequential.html">Sequential</a></span>(
    <span class="kw"><a href="../reference/For.html">For</a></span>(<span class="dv">1</span><span class="op">:</span>num_layers, <span class="cf">function</span>() {<span class="kw">c</span>(
      <span class="kw"><a href="../reference/Sequential.html">Sequential</a></span>(<span class="kw"><a href="../reference/Stabilizer.html">Stabilizer</a></span>(), <span class="kw"><a href="../reference/Recurrence.html">Recurrence</a></span>(<span class="kw"><a href="../reference/LSTM.html">LSTM</a></span>(hidden_dim), <span class="dt">go_backwards =</span> <span class="ot">FALSE</span>))
    )}),
    <span class="kw"><a href="../reference/Dense.html">Dense</a></span>(output_dim)
  )
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">create_inputs &lt;-<span class="st"> </span><span class="cf">function</span>(vocab_dim) {
  input_seq_axis &lt;-<span class="st"> </span><span class="kw"><a href="../reference/CNTKAxis.html">CNTKAxis</a></span>(<span class="st">'inputAxis'</span>)
  input_sequence &lt;-<span class="st"> </span><span class="kw"><a href="../reference/seq_input_variable.html">seq_input_variable</a></span>(<span class="dt">shape =</span> vocab_dim, <span class="dt">sequence_axis =</span> input_seq_axis, <span class="dt">name =</span> <span class="st">'input'</span>)
  label_sequence &lt;-<span class="st"> </span><span class="kw"><a href="../reference/seq_input_variable.html">seq_input_variable</a></span>(<span class="dt">shape =</span> vocab_dim, <span class="dt">sequence_axis =</span> input_seq_axis, <span class="dt">name =</span> <span class="st">'label'</span>)
  
  <span class="kw">list</span>(input_sequence, label_sequence)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_lm &lt;-<span class="st"> </span><span class="cf">function</span>(training_file, epochs, max_num_minibatches) {
  <span class="co"># load data and vocab</span>
  l &lt;-<span class="st"> </span><span class="kw">load_data_and_vocab</span>(training_file)
  data &lt;-<span class="st"> </span>l[<span class="dv">1</span>]
  char_to_ix &lt;-<span class="st"> </span>l[<span class="dv">2</span>]
  data_size &lt;-<span class="st"> </span>l[<span class="dv">3</span>]
  vocab_dim &lt;-<span class="st"> </span>l[<span class="dv">4</span>]
  
  <span class="co"># model the source targets inputs to the model</span>
  sequences &lt;-<span class="st"> </span><span class="kw">create_inputs</span>(vocab_dim)
  input_sequence &lt;-<span class="st"> </span>sequences[<span class="dv">1</span>]
  label_sequence &lt;-<span class="st"> </span>sequences[<span class="dv">2</span>]
  
  <span class="co"># create the model and apply to input sequence</span>
  model &lt;-<span class="st"> </span><span class="kw">create_model</span>(vocab_dim)
  z &lt;-<span class="st"> </span><span class="kw">model</span>(input_sequence)
  
  <span class="co"># setup criteria</span>
  loss &lt;-<span class="st"> </span><span class="kw"><a href="../reference/loss_cross_entropy_with_softmax.html">loss_cross_entropy_with_softmax</a></span>(z, label_sequence)
  error &lt;-<span class="st"> </span><span class="kw"><a href="../reference/classification_error.html">classification_error</a></span>(z, label_sequence)
  
  <span class="co"># instantiate trainer object</span>
  lr_per_sample &lt;-<span class="st"> </span><span class="kw">learning_rate_schedule</span>(<span class="fl">0.001</span>, <span class="kw">UnitType</span>(<span class="st">'sample'</span>))
  momentum_time_constant &lt;-<span class="st"> </span><span class="kw">momentum_as_time_constant_schedule</span>(<span class="dv">1100</span>)
  learner &lt;-<span class="st"> </span><span class="kw"><a href="../reference/learner_momentum_sgd.html">learner_momentum_sgd</a></span>(z<span class="op">$</span>parameters, lr_per_sample, momentum_time_constant,
                                  <span class="dt">gradient_clipping_threshold_per_sample =</span> <span class="dv">5</span>,
                                  <span class="dt">gradient_clipping_with_truncation =</span> <span class="ot">TRUE</span>)
  progress_printer &lt;-<span class="st"> </span><span class="kw"><a href="../reference/ProgressPrinter.html">ProgressPrinter</a></span>(<span class="dt">freq =</span> <span class="dv">100</span>, <span class="dt">tag =</span> <span class="st">'Training'</span>)
  trainer &lt;-<span class="st"> </span><span class="kw"><a href="../reference/Trainer.html">Trainer</a></span>(z, <span class="kw">c</span>(loss, error), learner, progress_printer)
  
  sample_freq &lt;-<span class="st"> </span><span class="dv">1000</span>
  minibatches_per_epoch &lt;-<span class="st"> </span><span class="kw">min</span>(<span class="kw">floor</span>(data_size <span class="op">/</span><span class="st"> </span>minibatch_size),
                               <span class="kw">floor</span>(max_num_minibatches <span class="op">/</span><span class="st"> </span>epochs))
  
  <span class="co"># print out some useful training information</span>
  <span class="kw"><a href="../reference/log_number_of_parameters.html">log_number_of_parameters</a></span>(z)
  <span class="kw">sprintf</span>(<span class="st">"Running %d epochs with %d minibatches per epoch</span><span class="ch">\n</span><span class="st">"</span>, epochs, minibatches_per_epoch)
  
  <span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epochs) {
    <span class="co"># Specify the mapping of input variables in the model to actual minibatch data to be trained with</span>
    <span class="co"># If it's the start of the data, we specify that we are looking at a new sequence (True)</span>
    mask =<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>)
    <span class="cf">for</span> (batch <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>minibatches_per_epoch) {
      minibatch &lt;-<span class="st"> </span><span class="kw">get_data</span>(batch, minibatch_size, data, char_to_ix, vocab_dim)
      arguments &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">list</span>(<span class="st">'input'</span> =<span class="st"> </span>minibatch[<span class="dv">1</span>], <span class="st">'label'</span> =<span class="st"> </span>minibatch[<span class="dv">2</span>]), mask)
      mask &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">FALSE</span>)
      trainer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../reference/train_minibatch.html">train_minibatch</a></span>(arguments)
      
      global_minibatch &lt;-<span class="st"> </span>epoch <span class="op">*</span><span class="st"> </span>minibatches_per_epoch <span class="op">+</span><span class="st"> </span>batch
      <span class="cf">if</span> (global_minibatch <span class="op">%%</span><span class="st"> </span>sample_freq <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
        <span class="kw">print</span>(<span class="kw">sample</span>(z, ix_to_char, vocab_dim, char_to_ix))
      }
    }
    
    model_filename &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">"models/shakespeare_epoch"</span>, <span class="kw">toString</span>(epoch <span class="op">+</span><span class="st"> </span><span class="dv">1</span>), <span class="st">".dnn"</span>, <span class="dt">sep =</span> <span class="st">""</span>)
    <span class="kw"><a href="../reference/func_save.html">func_save</a></span>(model_filename)
    <span class="kw">sprintf</span>(<span class="st">"Saved model to '%s'"</span>, model_filename)
  }
  
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">load_and_sample &lt;-<span class="st"> </span><span class="cf">function</span>(model_filename, vocab_filename, <span class="dt">prime_text =</span> <span class="st">''</span>, <span class="dt">use_hardmax =</span> <span class="ot">FALSE</span>,
                            <span class="dt">length =</span> <span class="dv">1000</span>, <span class="dt">temperature =</span> <span class="fl">1.0</span>) {
  model &lt;-<span class="st"> </span><span class="kw"><a href="../reference/func_load.html">func_load</a></span>(model_filename)
  
  <span class="co"># load vocab</span>
  char_to_ix &lt;-<span class="st"> </span><span class="kw">list</span>()
  ix_to_char &lt;-<span class="st"> </span><span class="kw">list</span>()
  chars &lt;-<span class="st"> </span><span class="kw">strsplit</span>(<span class="kw">readChar</span>(vocab_filename, <span class="kw">file.info</span>(vocab_filename)<span class="op">$</span>size))[[<span class="dv">1</span>]]
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(chars)) {
    char_to_ix[chars[i]] &lt;-<span class="st"> </span>i
    ix_to_char[<span class="kw">toString</span>(i)] &lt;-<span class="st"> </span>chars[i]
  }
  
  <span class="kw">sample</span>(model, ix_to_char, <span class="kw">length</span>(chars), char_to_ix, <span class="dt">prime_text =</span> prime_text, <span class="dt">use_hardmax =</span> use_hardmax,
         <span class="dt">length =</span> length, <span class="dt">temperature =</span> temperature)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">epochs &lt;-<span class="st"> </span><span class="dv">50</span>
max_num_minibatches &lt;-<span class="st"> </span>.Machine<span class="op">$</span>integer.max
<span class="kw">train_lm</span>(<span class="st">"../example-data/tinyshakespeare.txt"</span>, epochs, max_num_minibatches)

model_path &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">"../models/shakespeare_epoch"</span>, <span class="kw">toString</span>(epochs), <span class="st">".dnn"</span>, <span class="dt">sep =</span> <span class="st">""</span>)
vocab_path &lt;-<span class="st"> "../example-data/tinyshakespeare.txt.vocab"</span>

output &lt;-<span class="st"> </span><span class="kw">load_and_sample</span>(model_path, vocab_path, <span class="dt">prime_text =</span> <span class="st">'T'</span>, <span class="dt">use_hardmax =</span> <span class="ot">FALSE</span>,
                          <span class="dt">length =</span> <span class="dv">100</span>, <span class="dt">temperature =</span> <span class="fl">0.95</span>)

<span class="kw">write</span>(<span class="st">'output.txt'</span>, output)</code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#goals-and-overview">Goals and Overview</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Ali Zaidi, Joe Davison, Microsoft.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
